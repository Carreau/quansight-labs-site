<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="assets/xml/rss.xsl" media="all"?><rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Quansight Labs</title><link>https://labs.quansight.org/</link><description>Quansight Labs site: blog, development and project overview</description><atom:link href="https://labs.quansight.org/rss.xml" rel="self" type="application/rss+xml"></atom:link><language>en</language><copyright>Contents © 2021 &lt;a href="mailto:info@quansight.com"&gt;Quansight Labs Team&lt;/a&gt; </copyright><lastBuildDate>Fri, 07 May 2021 21:48:55 GMT</lastBuildDate><generator>Nikola (getnikola.com)</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>Rethinking Jupyter Interactive Documentation</title><link>https://labs.quansight.org/blog/2021/05/rethinking-jupyter-documentation/</link><dc:creator>Matthias Bussonnier</dc:creator><description>&lt;div&gt;&lt;p&gt;Jupyter Notebook first release was 8 years ago – under the IPython Notebook
name at the time. Even if notebooks were not invented by Jupyter; they were
definitely democratized by it. Being Web powered allowed development of many
changes in the Datascience world. Objects now often expose rich representation; from
Pandas dataframes with as html tables, to more recent &lt;a href="https://github.com/scikit-learn/scikit-learn/pull/14180"&gt;Scikit-learn model&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Today I want to look into a topic that has not evolved much since, and I believe
could use an upgrade. Accessing interactive Documentation when in a Jupyter
session, and what it could become. At the end I'll link to my current prototype
if you are adventurous.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://labs.quansight.org/blog/2021/05/rethinking-jupyter-documentation/"&gt;Read more…&lt;/a&gt; (7 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>documentation</category><category>Open-Source</category><category>Python</category><guid>https://labs.quansight.org/blog/2021/05/rethinking-jupyter-documentation/</guid><pubDate>Fri, 07 May 2021 00:01:00 GMT</pubDate></item><item><title>Spot the differences: what is new in Spyder 5?</title><link>https://labs.quansight.org/blog/2021/04/spot-the-diffenrences/</link><dc:creator>Isabela Presedo-Floyd and Juanita Gomez</dc:creator><description>&lt;div&gt;&lt;p&gt;&lt;img alt="Spyder 5 versus Spyder 4" src="https://labs.quansight.org/images/spyder5-header.png"&gt;&lt;/p&gt;
&lt;p&gt;In case you missed it, Spyder 5 was released at the beginning of April! This 
blog post is a conversation attempting to document the long and complex 
process of improving Spyder's UI with this release. Portions lead by Juanita 
Gomez are marked as &lt;strong&gt;Juanita&lt;/strong&gt;, and those lead by Isabela Presedo-Floyd are 
marked as &lt;strong&gt;Isabela&lt;/strong&gt;.&lt;/p&gt;
&lt;h3&gt;What did we do?&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;[Juanita]&lt;/strong&gt; &lt;a href="https://www.spyder-ide.org/"&gt;Spyder&lt;/a&gt; was created more than 10 
years ago and it has had the contributions of a great number of developers 
who have written code, proposed ideas, opened issues and tested PRs in order 
to build a piece of Spyder on their own. We (the Spyder team) have been lucky 
to have such a great community of people contributing throughout the years, 
but this is the first time that we decided to ask for help from an UX/UI 
expert! Why? You might wonder. Having the contributions of this great amount 
of people has resulted in inconsistencies around Spyder’s interface which we 
didn’t stop to analyze until now. &lt;/p&gt;
&lt;p&gt;When Isabela joined Quansight, we realized that we had an opportunity of 
improving Spyder’s interface with her help. We thought her skill set was 
everything we needed to make Spyder’s UI better.  So we started by reviewing 
the results of a community survey from a few months ago and realized that 
some of the most common feedback from users is related to its interface 
(very crowded, not consistent, many colors). This is why we decided to start 
a joint project with Isabela, (who we consider now part of the Spyder team) 
called &lt;a href="https://github.com/spyder-ide/spyder/releases/tag/v5.0.0"&gt;Spyder 5&lt;/a&gt;!!!&lt;/p&gt;
&lt;p&gt;&lt;a href="https://labs.quansight.org/blog/2021/04/spot-the-diffenrences/"&gt;Read more…&lt;/a&gt; (8 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>release</category><category>Spyder</category><category>UX/UI</category><guid>https://labs.quansight.org/blog/2021/04/spot-the-diffenrences/</guid><pubDate>Fri, 16 Apr 2021 14:00:00 GMT</pubDate></item><item><title>A step towards educating with Spyder</title><link>https://labs.quansight.org/blog/2021/04/a-step-towards-educating-with-spyder/</link><dc:creator>Juanita Gomez</dc:creator><description>&lt;div&gt;&lt;p&gt;As a community manager in the Spyder team, I have been looking for ways of
involving more users in the community and making Spyder useful for a larger
number of people. With this, a new idea came: Education.&lt;/p&gt;
&lt;p&gt;For the past months, we have been wondering with the team whether Spyder
could also serve as a teaching-learning platform, especially in this era
where remote instruction has become necessary. We submitted a proposal to the
Essential Open Source Software for Science (EOSS) program of the Chan
Zuckerberg Initiative, during its third cycle, with the idea of providing a
simple way inside Spyder to create and share interactive tutorials on topics
relevant to scientific research. Unfortunately, we didn’t get this funding,
but we didn’t let this great idea die.&lt;/p&gt;
&lt;p&gt;We submitted a second proposal to the &lt;a href="https://www.python.org/psf/"&gt;Python Software Foundation&lt;/a&gt;
from which we were awarded $4000. For me, this is the perfect opportunity for
us to take the first step towards using Spyder for education.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://labs.quansight.org/blog/2021/04/a-step-towards-educating-with-spyder/"&gt;Read more…&lt;/a&gt; (2 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>community</category><category>funding</category><category>grant</category><category>Spyder</category><guid>https://labs.quansight.org/blog/2021/04/a-step-towards-educating-with-spyder/</guid><pubDate>Sun, 11 Apr 2021 14:00:00 GMT</pubDate></item><item><title>PyTorch TensorIterator Internals - 2021 Update</title><link>https://labs.quansight.org/blog/2021/04/pytorch-tensoriterator-internals-update/</link><dc:creator>Kurt Mohler</dc:creator><description>&lt;div&gt;&lt;p&gt;For contributors to the PyTorch codebase, one of the most commonly encountered
C++ classes is
&lt;a href="https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/TensorIterator.h"&gt;&lt;code&gt;TensorIterator&lt;/code&gt;&lt;/a&gt;.
&lt;code&gt;TensorIterator&lt;/code&gt; offers a standardized way to iterate over elements of
a tensor, automatically parallelizing operations, while abstracting device and
data type details.&lt;/p&gt;
&lt;p&gt;In April 2020, Sameer Deshmukh wrote a blog article discussing
&lt;a href="https://labs.quansight.org/blog/2020/04/pytorch-tensoriterator-internals/index.html"&gt;PyTorch TensorIterator Internals&lt;/a&gt;. Recently,
however, the interface has changed significantly. This post describes how to
use the current interface as of April 2021. Much of the information from the
previous article is directly copied here, but with updated API calls and some
extra details.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://labs.quansight.org/blog/2021/04/pytorch-tensoriterator-internals-update/"&gt;Read more…&lt;/a&gt; (8 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><guid>https://labs.quansight.org/blog/2021/04/pytorch-tensoriterator-internals-update/</guid><pubDate>Fri, 09 Apr 2021 14:00:00 GMT</pubDate></item><item><title>Accessibility: Who's Responsible?</title><link>https://labs.quansight.org/blog/2021/03/accessibility-whos-responsible/</link><dc:creator>Isabela Presedo-Floyd</dc:creator><description>&lt;div&gt;&lt;p&gt;&lt;img alt="Fingers and question marks pointing in every direction" src="https://labs.quansight.org/images/jlabaccess1.png"&gt;&lt;/p&gt;
&lt;h3&gt;JupyterLab Accessibility Journey Part 1&lt;/h3&gt;
&lt;p&gt;For the past few months, I've been part of a group of people in the JupyterLab community 
who've committed to start chipping away at the many accessibility failings of JupyterLab. 
I find this work is critical, fascinating, and a learning experience for everyone involved. 
So I'm going to document my personal experience and lessons I've learned in a series of blog 
posts. Welcome!&lt;/p&gt;
&lt;p&gt;&lt;a href="https://labs.quansight.org/blog/2021/03/accessibility-whos-responsible/"&gt;Read more…&lt;/a&gt; (6 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>Accessibility</category><category>JLabA11y</category><category>JupyterLab</category><guid>https://labs.quansight.org/blog/2021/03/accessibility-whos-responsible/</guid><pubDate>Thu, 25 Mar 2021 08:00:00 GMT</pubDate></item><item><title>Enhancements to Numba's guvectorize decorator</title><link>https://labs.quansight.org/blog/2021/02/enhancements-to-numba-guvectorize-decorator/</link><dc:creator>Guilherme Leobas</dc:creator><description>&lt;div&gt;&lt;p&gt;Starting from Numba 0.53, Numba will ship with an enhanced version of the &lt;code&gt;@guvectorize&lt;/code&gt; decorator. Similar to the &lt;a href="https://numba.pydata.org/numba-doc/dev/user/vectorize.html#the-vectorize-decorator"&gt;@vectorize&lt;/a&gt; decorator, &lt;a href="https://numba.pydata.org/numba-doc/dev/user/vectorize.html#the-guvectorize-decorator"&gt;@guvectorize&lt;/a&gt; now has two modes of operation:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Eager, or decoration-time compilation and&lt;/li&gt;
&lt;li&gt;Lazy, or call-time compilation&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Before, only the eager approach was supported. In this mode, users are required to provide a list of concrete supported types beforehand as its first argument. Now, this list can be omitted if desired and as one calls it, Numba dynamically generates new kernels for previously unsupported types.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://labs.quansight.org/blog/2021/02/enhancements-to-numba-guvectorize-decorator/"&gt;Read more…&lt;/a&gt; (3 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>Labs</category><category>Numba</category><guid>https://labs.quansight.org/blog/2021/02/enhancements-to-numba-guvectorize-decorator/</guid><pubDate>Thu, 25 Feb 2021 08:00:00 GMT</pubDate></item><item><title>Python packaging in 2021 - pain points and bright spots</title><link>https://labs.quansight.org/blog/2021/01/python-packaging-brainstorm/</link><dc:creator>Ralf Gommers</dc:creator><description>&lt;div&gt;&lt;p&gt;At Quansight we have a weekly "Q-share" session on Fridays where everyone can
share/demo things they have worked on, recently learned, or that simply seem
interesting to share with their colleagues. This can be about anything, from
new utilities to low-level performance, from building inclusive communities
to how to write better documentation, from UX design to what legal &amp;amp;
accounting does to support the business. This week I decided to try something
different: hold a brainstorm on the state of Python packaging today.&lt;/p&gt;
&lt;p&gt;The ~30 participants were mostly from the PyData world, but not exclusively -
it included people with backgrounds and preferences ranging from C, C++ and
Fortran to JavaScript, R and DevOps - and with experience as end-users,
packagers, library authors, and educators. This blog post contains the raw
output of the 30-minute brainstorm (only cleaned up for textual issues) and
my annotations on it (in italics) which capture some of the discussion during
the session and links and context that may be helpful. I think it sketches a
decent picture of the main pain points of Python packaging for users and
developers interacting with the Python data and numerical computing ecosystem.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://labs.quansight.org/blog/2021/01/python-packaging-brainstorm/"&gt;Read more…&lt;/a&gt; (11 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>Conda</category><category>conda-forge</category><category>CUDA</category><category>manylinux</category><category>packaging</category><category>pip</category><category>PyData</category><category>PyPI</category><category>Python</category><category>setuptools</category><guid>https://labs.quansight.org/blog/2021/01/python-packaging-brainstorm/</guid><pubDate>Sun, 24 Jan 2021 04:00:00 GMT</pubDate></item><item><title>Making SciPy's Image Interpolation Consistent and Well Documented</title><link>https://labs.quansight.org/blog/2021/01/scipy-ndimage-interpolation/</link><dc:creator>Gregory Lee</dc:creator><description>&lt;div&gt;&lt;h2&gt;SciPy n-dimensional Image Processing&lt;/h2&gt;
&lt;p&gt;SciPy's ndimage module provides a powerful set of general, n-dimensional image processing operations, categorized into areas such as filtering, interpolation and morphology. Traditional image processing deals with 2D arrays of pixels, possibly with an additional array dimension of size 3 or 4 to represent color channel and transparency information. However, there are many scientific applications where we may want to work with more general arrays such as the 3D volumetric images produced by medical imaging methods like computed tomography (CT) or magnetic resonance imaging (MRI) or biological imaging approaches such as light sheet microscopy. Aside from spatial axes, such data may have additional axes representing other quantities such as time, color, spectral frequency or different contrasts. Functions in ndimage have been implemented in a general n-dimensional manner so that they can be applied across 2D, 3D or more dimensions. A more detailed overview of the module is available in the
&lt;a href="https://docs.scipy.org/doc/scipy/reference/tutorial/ndimage.html"&gt;SciPy ndimage tutorial&lt;/a&gt;. SciPy's image functions are also used by downstream libraries such as &lt;a href="https://scikit-image.org"&gt;scikit-image&lt;/a&gt; to implement higher-level algorithms for things like image restoration, segmentation and registration.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://labs.quansight.org/blog/2021/01/scipy-ndimage-interpolation/"&gt;Read more…&lt;/a&gt; (5 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>Labs</category><category>Open-Source</category><category>Python</category><category>SciPy</category><guid>https://labs.quansight.org/blog/2021/01/scipy-ndimage-interpolation/</guid><pubDate>Fri, 22 Jan 2021 14:00:00 GMT</pubDate></item><item><title>Welcoming Tania Allard as Quansight Labs co-director</title><link>https://labs.quansight.org/blog/2021/01/welcoming-tania-allard-labs-codirector/</link><dc:creator>Ralf Gommers</dc:creator><description>&lt;div&gt;&lt;p&gt;&lt;img alt="Photo Tania Allard" src="https://labs.quansight.org/galleries/team/tania_allard.jpg"&gt;&lt;/p&gt;
&lt;p&gt;Today I'm incredibly excited to welcome Tania Allard to Quansight as
Co-Director of Quansight Labs. Tania (&lt;a href="https://github.com/trallard"&gt;GitHub&lt;/a&gt;,
&lt;a href="https://twitter.com/ixek/"&gt;Twitter&lt;/a&gt;, &lt;a href="https://www.bitsandchips.me/"&gt;personal
site&lt;/a&gt;) is a well-known and prolific PyData
community member. In the past few years she has been involved as a conference
organizer (JupyterCon, SciPy, PyJamas, PyCon UK, PyCon LatAm, JuliaCon and
more), as a community builder (PyLadies, NumFOCUS, RForwards), as a
contributor to Matplotlib and Jupyter, and as a regular speaker and mentor.
She also brings relevant experience in both industry and academia - she joins
us from Microsoft where she was a senior developer advocate, and has a PhD in
computational modelling.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://labs.quansight.org/blog/2021/01/welcoming-tania-allard-labs-codirector/"&gt;Read more…&lt;/a&gt; (4 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>Labs</category><guid>https://labs.quansight.org/blog/2021/01/welcoming-tania-allard-labs-codirector/</guid><pubDate>Mon, 04 Jan 2021 08:00:00 GMT</pubDate></item><item><title>Develop a JupyterLab Winter Theme</title><link>https://labs.quansight.org/blog/2020/12/jupyterlab-winter-theme/</link><dc:creator>Matthias Bussonnier, Isabela Presedo Floyd, Eric Charles, Eric Kelly, Tony Fast</dc:creator><description>&lt;div&gt;&lt;p&gt;JupyterLab 3.0 is about to be released and provides many 
improvements to the extension system. Theming is a way to extend JupyterLab and 
benefits from those improvements.&lt;/p&gt;
&lt;p&gt;While theming is often disregarded as a purely cosmetic endeavour, it can greatly
improve software. Theming can be great help for accessibility, and the Jupyter team
pays attention to making the default appearance accessibility-aware by using
sufficient contrast.  For users with a high visual acuity you may also choose 
to increase the information density.&lt;/p&gt;
&lt;p&gt;Theming can also be a great way to improve communication by increasing or
decreasing emphasis of the user interface, which can be of use for teaching or
presenting. Theming may also help with security, for example, by having a clear
distinction between staging and production.&lt;/p&gt;
&lt;p&gt;Finally Theming can be a great way to express oneself, for example, by using
a branded version of software that fits well into a context, or expressing one's artistic
preferences or opinions. &lt;/p&gt;
&lt;p&gt;In the following blog post, we will show you step-by-step how you can
develop a custom theme for JupyterLab, distribute it, and take the example of the
&lt;a href="https://github.com/Quansight-Labs/jupyterlab-theme-winter"&gt;jupyterlab-theme-winter&lt;/a&gt; theme we release today to celebrate the end of 2020.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://labs.quansight.org/blog/2020/12/jupyterlab-winter-theme/"&gt;Read more…&lt;/a&gt; (9 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>Jupyter</category><category>JupyterLab</category><category>JupyterTutorials</category><category>Labs</category><category>Theme</category><guid>https://labs.quansight.org/blog/2020/12/jupyterlab-winter-theme/</guid><pubDate>Tue, 22 Dec 2020 09:00:00 GMT</pubDate></item></channel></rss>